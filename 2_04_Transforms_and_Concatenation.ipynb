{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1xqQczl0FG-qtNA2_WQYuWePW9oU8irqJ)"
      ],
      "metadata": {
        "id": "kIqlw0aJzOuh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.04 Pandas: Transforms and concatenation\n",
        "### Concatenation\n",
        "In the previous section we worked with data we created in-session to make two data frames we then merged - joining by columns. In this session we will work in an opposite way: firstly we will read in data from file rather than creating it; then, secondly, we will join datasets via concatenation - joining by rows (shared columns).\n",
        "\n",
        "To being with we need to access the data. However, we will make things slightly trickier for ourselves by working with an Excel with multiple worksheets. From my.wbs, asynchronous tasks for week 3, you will find a file called \"fake_sales.xlsx\". The file contains sales data across three locations (the US, UK and Canada) for January and February (one tab for each month). Save it on your computer.\n",
        "\n",
        "The first task is to move the file to Colab. Run this code to open a file picker to find the file:"
      ],
      "metadata": {
        "id": "S1DJUspsANB4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYnW1HS6AG6-"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "upload = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’ll then need to read in this data, and we’ll need to do this one worksheet at a time. Let’s start with January:"
      ],
      "metadata": {
        "id": "R-Fwe5yrAY1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd # if not already done in your session\n",
        "import numpy as np\n",
        "\n",
        "jan_df = pd.read_excel('fake_sales.xlsx', sheet_name='January')\n",
        "jan_df"
      ],
      "metadata": {
        "id": "_5S9uyHOBuca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As shown, we can inspect the data in full to ensure it has imported correctly. However at 31 rows it starts to become a lot of data to review, and in the realworld its likely our data would be much bigger. Let’s use the _head( )_ function to view just the top five rows:"
      ],
      "metadata": {
        "id": "llHg1B3AB8Mn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jan_df.head()"
      ],
      "metadata": {
        "id": "Q49bx2psCCPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Everything looks fine but let's check the data types/dtypes to be sure:"
      ],
      "metadata": {
        "id": "T-UmbL1KCFOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(jan_df.dtypes)"
      ],
      "metadata": {
        "id": "DonIJFa-CIXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Everything looks as expected! Let's do the same for the 'Feb' worksheet."
      ],
      "metadata": {
        "id": "f1L-eOc1CLUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feb_df = pd.read_excel('fake_sales.xlsx', sheet_name='February')\n",
        "feb_df.head()"
      ],
      "metadata": {
        "id": "kga7XihBCOR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now want to join the data. However, unlike in the previous example we don’t want to join as new columns, but as new rows. This means we use a different type of join than the _merge( )_ we did in the previous session. Instead we will use concatenation – i.e. we will add the February rows to the bottom of the January rows. However, we have one small issue – the column names are not exactly the same. The January file, for instance, shows “US_Sales” while the February file has “US Sales”. To join the two we will need to correct this. We do this with the following code:"
      ],
      "metadata": {
        "id": "bwiyxlmCCTQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feb_df.columns = ['Date', 'US_Sales', 'UK_Sales', 'Canada_Sales']\n",
        "feb_df.head()"
      ],
      "metadata": {
        "id": "yG8XJDjOCXGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this change made we can execute the concatenation code. We are using _pd.concat( )_ (pandas' concatenation function, in which we specify the dataframes to join as a list, the axis we want to join (0 for rows (which we want) or 1 for columns) and the type of join. “Left join” and “right join” are not relevant in this case, as we are joining at the bottom not the sides, but we can choose a join of “inner” to ignore columns not in both sets, or “outer” to include all columns irrespective of whether they are in all the Dataframes. Actually in this case it is irrelevant because the columns are both the same."
      ],
      "metadata": {
        "id": "-4IisW_iCjGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_df = pd.concat([jan_df, feb_df], axis=0, join='outer')\n",
        "sales_df.head()"
      ],
      "metadata": {
        "id": "cVRvh4dACpY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are no errors (yay!) but we can only see January data using head - which is unsuprising as our February data is meant to be concatenated to the bottom of the dataframe. Fortunately there is an equivalent function to _head( )_ to show the bottom five records (which should all be February data if everything has worked::"
      ],
      "metadata": {
        "id": "7PYNU8yDDFko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_df.tail()"
      ],
      "metadata": {
        "id": "az5kjr6GDMUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Transformations\n",
        "Data transformations cover a wide range of actions including basic cleaning up steps through to advanced mathematical operations. We'll mostly focus on the former here!\n",
        "\n",
        "Checking back to the Excel sheet we can see that these are blank cells with no data – missing data, \"NaNs\" or \"nulls\" are the common name. We can also verify this issues in pandas using the _isnull( )_ function:"
      ],
      "metadata": {
        "id": "PVdg8ranDSZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_df.isnull().sum()"
      ],
      "metadata": {
        "id": "k0jcj4O3DZ3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see there are a total of four null/missing values.\n",
        "\n",
        "Dealing with missing data is a common problem with no single “right” solution (it depends on the specifics of the case). There is an extensive article on the topic in the pandas documentation (https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html).\n",
        "\n",
        "In our (fake) scenario, we have followed the most sensible action – we checked with the people who produced the data. They have reported that because of system issues these were actually zero sales days and should be reported as such. We need, therefore to replace the “NaN” records with 0:"
      ],
      "metadata": {
        "id": "9CXHdnvkDkBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_df = sales_df.fillna(0)\n",
        "sales_df.isnull().sum()"
      ],
      "metadata": {
        "id": "ouzqEr1PDnWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sum of _isnull( )_ is now zero so everthing has worked.\n",
        "\n",
        "An additional problem is that we have multiple currencies in our dataframe. From the Excel sheet we see that US and Canada are in dollars (presumably US dollars and Canadian dollars respectively), and UK is in pounds. This will prevent us from comparing these values in a meaningful way. We want to convert all of these columns to the same currency – in this case UK pounds. Checking a currency exchange (https://www.xe.com/) we see that a US dollar is worth £0.76, and a Canadian dollar as £0.56. We need to apply these adjustments to our dataframe."
      ],
      "metadata": {
        "id": "TEEbCcqJD7sF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_df['US_Sales'] = sales_df['US_Sales'] * 0.76\n",
        "sales_df['Canada_Sales'] = sales_df['Canada_Sales'] * 0.56"
      ],
      "metadata": {
        "id": "FCD-4G17ERod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, what we want to do with this data is to calculate an average by month for each country. We can use pandas’ _groupby( )_ functionality to do this. However, we first need to establish which data is from February and which is from January (obviously it would have been easier to do this before merging, but for the sake of example). We need to create another calculated field, this time capturing the month:"
      ],
      "metadata": {
        "id": "t1DwrtS6EZ9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_df['Month'] = pd.DatetimeIndex(sales_df['Date']).month\n",
        "sales_df.head()"
      ],
      "metadata": {
        "id": "NWZHkakoECGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_df.tail()"
      ],
      "metadata": {
        "id": "-sx8cmMeEsif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code creates a new column with a pandas DatetimeIndex (an index number related to an aspect of datatime such as a day of the month would be the number and year would be a number), with the final part specifying we want the month index (1 or 2). Again, we can verify this in the notebook.\n",
        "\n",
        "This transformation achieves what we need, although maybe not in the prettiest way. To fix this up we will use another transformation to change the month record to \"Jan\" or \"Feb\":"
      ],
      "metadata": {
        "id": "Gy7FBUsmEp7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_df['Month'] = np.where(sales_df.Month == 1, 'Jan', 'Feb')\n",
        "sales_df.head()"
      ],
      "metadata": {
        "id": "cA88RvaNEzy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_df.tail()"
      ],
      "metadata": {
        "id": "0PtP-yvYCXmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This command uses numpy (np) – specifically the _where( )_ function that works very much like an if condition. We first apply a logical test (does each item in the \"Month\" column equal 1) and if True we change it to \"Jan\" and if False we change it to \"Feb\". Again this is not necessarily the most efficient way to achieve what we want but it does show some useful functionality.\n",
        "\n",
        "Finally we do our groupings. We will group by month to get the figures for each region. However, as we aggregate the figures we need to pick an appropriate method. This could be the maximum, the minimum, the median, or so on. In this case we will use the mean average:"
      ],
      "metadata": {
        "id": "KNce6v__E3gj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_group_df = sales_df.groupby(['Month']).mean()\n",
        "sales_group_df"
      ],
      "metadata": {
        "id": "R5ecKL5eE5ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen we now have aggregated/average sales for each day of each month for each of our regions. Good job!"
      ],
      "metadata": {
        "id": "KVeaPW2FE_rI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EXERCISE\n",
        "1) Re-do the last steps of the sales group process changing the _groupby( )_ function for sales figures to _median( )_ ... what difference does this make to the result? Would this be expected?\n",
        "\n"
      ],
      "metadata": {
        "id": "h7YtLMiyosYJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### More groupby Options\n",
        "We use the groupby function when we want to change the unit of aggregation of a DataFrame. For instance, we may have a DataFrame that has 1x row per day and we want to aggregate up to 1x row per month:"
      ],
      "metadata": {
        "id": "tWF3o0mlG2pB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({\"Team\": [\"DS\", \"DT\", \"DT\", \"DS\", \"DS\"], \"salary\": [100000, 100, 20000, 0, 100000]})\n",
        "print(\"Raw Data\")\n",
        "print(df)\n",
        "print(\"\\n\")\n",
        "\n",
        "# team based aggregation (average)\n",
        "# reset index means we convert the output to a flat 2D shape rather than leaving a gap\n",
        "mean_df = df.groupby(['Team']).mean().reset_index()\n",
        "\n",
        "print(\"Aggregation by Mean\")\n",
        "print(mean_df)\n",
        "print(\"\\n\")\n",
        "\n",
        "# team based aggregation (sum)\n",
        "sum_df = df.groupby(['Team']).sum().reset_index()\n",
        "\n",
        "print(\"Aggregation by Sum\")\n",
        "print(sum_df)\n",
        "print(\"\\n\")\n",
        "\n",
        "# team based aggregation (median)\n",
        "median_df = df.groupby(['Team']).median().reset_index()\n",
        "\n",
        "print(\"Aggregation by Median\")\n",
        "print(median_df)\n",
        "print(\"\\n\")\n",
        "\n",
        "# team based aggregation (maximum)\n",
        "max_df = df.groupby(['Team']).max().reset_index()\n",
        "\n",
        "print(\"Aggregation by Maximum\")\n",
        "print(max_df)\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "id": "WMdlaoBzG3oW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Time and Date\n",
        "In this last part we will use pandas' date/time functionality for data  transforms:"
      ],
      "metadata": {
        "id": "S8hX8NuVFDLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({\"id\": [3, 1, 2, 4, 10], \"salary\": [100000, 100, 20000, 0, 100000],\n",
        "                   \"dob\": [\"10/01/1950\", \"01/01/1990\", \"01/01/1980\", \"12/12/1981\", \"13/06/1981\"]})\n",
        "print(df)\n",
        "\n",
        "# convert a column which has a date stored as string, into a column\n",
        "# stored as date. dayfirst=True means we use the UK style of date\n",
        "# (DDMMYYYY) rather than US (MMDDYYYY)\n",
        "df['dob'] = pd.to_datetime(df['dob'], dayfirst=True)\n",
        "\n",
        "# from a date object create cols for year, month, day, hour and minute\n",
        "df['Year'] =  pd.DatetimeIndex(df['dob']).year\n",
        "df['Month'] = pd.DatetimeIndex(df['dob']).month\n",
        "df['Day'] = pd.DatetimeIndex(df['dob']).day\n",
        "df['Hour'] = pd.DatetimeIndex(df['dob']).hour\n",
        "df['Minute'] = pd.DatetimeIndex(df['dob']).minute\n",
        "\n",
        "print(\"\\n\")\n",
        "print(df)\n",
        "\n",
        "# work out age by substracting the DOB from the date today\n",
        "# note this is much harder than you may think it would be because years\n",
        "# are different lengths and so ambiguous\n",
        "df[\"Age\"] = pd.to_datetime(\"today\", dayfirst=True)-df['dob']\n",
        "df[\"Age\"] = round(df.Age.dt.days / 365, 1)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(df)"
      ],
      "metadata": {
        "id": "I1C17K8no8Lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As an additional example:"
      ],
      "metadata": {
        "id": "5ApLRMA-pHDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({\"student\": [1, 2, 3, 4, 5],\n",
        "                   \"deadline\": [\"01/11/2025\", \"02/11/2025\", \"03/11/2025\", \"04/11/2025\", \"05/11/2025\"],\n",
        "                   \"submission\": [\"01/11/2025\", \"01/11/2025\", \"02/12/2025\", \"03/11/2025\", \"01/12/2025\"]})\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "dAiGLQ_IpJ0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert a column which has a date stored as string, into a column\n",
        "# stored as date. dayfirst=True means we use the UK style of date\n",
        "# (DDMMYYYY) rather than US (MMDDYYYY)\n",
        "# but this time inside a for loop\n",
        "\n",
        "date_cols = [\"deadline\", \"submission\"]\n",
        "\n",
        "for col in date_cols:\n",
        "  df[col] = pd.to_datetime(df[col], dayfirst=True)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "QBChzZLUp7jF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's look at when students submitted vs deadline:"
      ],
      "metadata": {
        "id": "shJhT3j8qKUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"after_deadline\"] = df[\"submission\"] - df[\"deadline\"]\n",
        "df.head()"
      ],
      "metadata": {
        "id": "UjgNNwN8qOSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__PUBLIC SERVICE ANNOUNCEMENT__: Don't submit after the deadline ... it's very expensive."
      ],
      "metadata": {
        "id": "h8y5XnrLqfiI"
      }
    }
  ]
}