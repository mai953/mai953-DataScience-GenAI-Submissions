{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mai953/mai953-DataScience-GenAI-Submissions/blob/main/6_02_DNN_101_Completed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1xqQczl0FG-qtNA2_WQYuWePW9oU8irqJ)"
      ],
      "metadata": {
        "id": "E0T9_-jFXxxc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.02 Dense Neural Network (with PyTorch)\n",
        "This will expand on our logistic regression example and take us through building our first neural network. If you haven't already, be sure to check (and if neccessary) switch to GPU processing by clicking Runtime > Change runtime type and selecting GPU. We can test this has worked with the following code:"
      ],
      "metadata": {
        "id": "dcEWDwlu94Xs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Check for GPU availability\n",
        "print(\"Num GPUs Available: \", torch.cuda.device_count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8cIpNbCvuQA",
        "outputId": "11b68b6a-a5b8-4820-ca61-a358f3c2e8cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hopefully your code shows you have 1 GPU available! Next let's get some data. We'll start with another in-built dataset:"
      ],
      "metadata": {
        "id": "8d6FF1wK-ph8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# upload an in-built Python (OK semi-in-built) dataset\n",
        "from sklearn.datasets import load_diabetes\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# import the data\n",
        "data = load_diabetes()\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MziWWXu-0ur",
        "outputId": "b173f595-fa40-4d86-a3ab-d584e4ac1c27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'data': array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,\n",
              "          0.01990749, -0.01764613],\n",
              "        [-0.00188202, -0.04464164, -0.05147406, ..., -0.03949338,\n",
              "         -0.06833155, -0.09220405],\n",
              "        [ 0.08529891,  0.05068012,  0.04445121, ..., -0.00259226,\n",
              "          0.00286131, -0.02593034],\n",
              "        ...,\n",
              "        [ 0.04170844,  0.05068012, -0.01590626, ..., -0.01107952,\n",
              "         -0.04688253,  0.01549073],\n",
              "        [-0.04547248, -0.04464164,  0.03906215, ...,  0.02655962,\n",
              "          0.04452873, -0.02593034],\n",
              "        [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,\n",
              "         -0.00422151,  0.00306441]]),\n",
              " 'target': array([151.,  75., 141., 206., 135.,  97., 138.,  63., 110., 310., 101.,\n",
              "         69., 179., 185., 118., 171., 166., 144.,  97., 168.,  68.,  49.,\n",
              "         68., 245., 184., 202., 137.,  85., 131., 283., 129.,  59., 341.,\n",
              "         87.,  65., 102., 265., 276., 252.,  90., 100.,  55.,  61.,  92.,\n",
              "        259.,  53., 190., 142.,  75., 142., 155., 225.,  59., 104., 182.,\n",
              "        128.,  52.,  37., 170., 170.,  61., 144.,  52., 128.,  71., 163.,\n",
              "        150.,  97., 160., 178.,  48., 270., 202., 111.,  85.,  42., 170.,\n",
              "        200., 252., 113., 143.,  51.,  52., 210.,  65., 141.,  55., 134.,\n",
              "         42., 111.,  98., 164.,  48.,  96.,  90., 162., 150., 279.,  92.,\n",
              "         83., 128., 102., 302., 198.,  95.,  53., 134., 144., 232.,  81.,\n",
              "        104.,  59., 246., 297., 258., 229., 275., 281., 179., 200., 200.,\n",
              "        173., 180.,  84., 121., 161.,  99., 109., 115., 268., 274., 158.,\n",
              "        107.,  83., 103., 272.,  85., 280., 336., 281., 118., 317., 235.,\n",
              "         60., 174., 259., 178., 128.,  96., 126., 288.,  88., 292.,  71.,\n",
              "        197., 186.,  25.,  84.,  96., 195.,  53., 217., 172., 131., 214.,\n",
              "         59.,  70., 220., 268., 152.,  47.,  74., 295., 101., 151., 127.,\n",
              "        237., 225.,  81., 151., 107.,  64., 138., 185., 265., 101., 137.,\n",
              "        143., 141.,  79., 292., 178.,  91., 116.,  86., 122.,  72., 129.,\n",
              "        142.,  90., 158.,  39., 196., 222., 277.,  99., 196., 202., 155.,\n",
              "         77., 191.,  70.,  73.,  49.,  65., 263., 248., 296., 214., 185.,\n",
              "         78.,  93., 252., 150.,  77., 208.,  77., 108., 160.,  53., 220.,\n",
              "        154., 259.,  90., 246., 124.,  67.,  72., 257., 262., 275., 177.,\n",
              "         71.,  47., 187., 125.,  78.,  51., 258., 215., 303., 243.,  91.,\n",
              "        150., 310., 153., 346.,  63.,  89.,  50.,  39., 103., 308., 116.,\n",
              "        145.,  74.,  45., 115., 264.,  87., 202., 127., 182., 241.,  66.,\n",
              "         94., 283.,  64., 102., 200., 265.,  94., 230., 181., 156., 233.,\n",
              "         60., 219.,  80.,  68., 332., 248.,  84., 200.,  55.,  85.,  89.,\n",
              "         31., 129.,  83., 275.,  65., 198., 236., 253., 124.,  44., 172.,\n",
              "        114., 142., 109., 180., 144., 163., 147.,  97., 220., 190., 109.,\n",
              "        191., 122., 230., 242., 248., 249., 192., 131., 237.,  78., 135.,\n",
              "        244., 199., 270., 164.,  72.,  96., 306.,  91., 214.,  95., 216.,\n",
              "        263., 178., 113., 200., 139., 139.,  88., 148.,  88., 243.,  71.,\n",
              "         77., 109., 272.,  60.,  54., 221.,  90., 311., 281., 182., 321.,\n",
              "         58., 262., 206., 233., 242., 123., 167.,  63., 197.,  71., 168.,\n",
              "        140., 217., 121., 235., 245.,  40.,  52., 104., 132.,  88.,  69.,\n",
              "        219.,  72., 201., 110.,  51., 277.,  63., 118.,  69., 273., 258.,\n",
              "         43., 198., 242., 232., 175.,  93., 168., 275., 293., 281.,  72.,\n",
              "        140., 189., 181., 209., 136., 261., 113., 131., 174., 257.,  55.,\n",
              "         84.,  42., 146., 212., 233.,  91., 111., 152., 120.,  67., 310.,\n",
              "         94., 183.,  66., 173.,  72.,  49.,  64.,  48., 178., 104., 132.,\n",
              "        220.,  57.]),\n",
              " 'frame': None,\n",
              " 'DESCR': '.. _diabetes_dataset:\\n\\nDiabetes dataset\\n----------------\\n\\nTen baseline variables, age, sex, body mass index, average blood\\npressure, and six blood serum measurements were obtained for each of n =\\n442 diabetes patients, as well as the response of interest, a\\nquantitative measure of disease progression one year after baseline.\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 442\\n\\n:Number of Attributes: First 10 columns are numeric predictive values\\n\\n:Target: Column 11 is a quantitative measure of disease progression one year after baseline\\n\\n:Attribute Information:\\n    - age     age in years\\n    - sex\\n    - bmi     body mass index\\n    - bp      average blood pressure\\n    - s1      tc, total serum cholesterol\\n    - s2      ldl, low-density lipoproteins\\n    - s3      hdl, high-density lipoproteins\\n    - s4      tch, total cholesterol / HDL\\n    - s5      ltg, possibly log of serum triglycerides level\\n    - s6      glu, blood sugar level\\n\\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of `n_samples` (i.e. the sum of squares of each column totals 1).\\n\\nSource URL:\\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\\n\\nFor more information see:\\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\\n',\n",
              " 'feature_names': ['age',\n",
              "  'sex',\n",
              "  'bmi',\n",
              "  'bp',\n",
              "  's1',\n",
              "  's2',\n",
              "  's3',\n",
              "  's4',\n",
              "  's5',\n",
              "  's6'],\n",
              " 'data_filename': 'diabetes_data_raw.csv.gz',\n",
              " 'target_filename': 'diabetes_target.csv.gz',\n",
              " 'data_module': 'sklearn.datasets.data'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are working on a regression problem, with \"structured\" data which has already been cleaned and normalised. We can skip the usual cleaning/engineering steps. However, we do need to get the data into PyTorch:"
      ],
      "metadata": {
        "id": "cZKrbx70_cIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to PyTorch tensors\n",
        "X = torch.tensor(data.data, dtype=torch.float32)\n",
        "y = torch.tensor(data.target, dtype=torch.float32).reshape(-1, 1) # Reshape y to be a column vector"
      ],
      "metadata": {
        "id": "f9PHiljr73fI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now our data is stored in tensors we can do train/test splitting as before (in fact we can use sklearn as before):"
      ],
      "metadata": {
        "id": "hu8VH2_SAOoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYJN01DV8Fac",
        "outputId": "403a932b-ca38-4a4d-837b-0b27e74efed2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([353, 10]) torch.Size([353, 1])\n",
            "torch.Size([89, 10]) torch.Size([89, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can set up our batches for training. As we have a nice round 400 let's go with batches of 50 (8 batches in total). We'll also seperate the features and labels:"
      ],
      "metadata": {
        "id": "LKmbZoCrJijU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Create TensorDatasets and DataLoaders\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=50, shuffle=False)"
      ],
      "metadata": {
        "id": "de0uOko08d-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now its time to build our model. We'll keep it simple ... a model with an input layer of 10 features and then 2x _Dense_ (fully connected) layers each with 5 neurons and ReLU activation. Our output layer will be size=1 given this is a regression problem and we want a single value output per prediction.\n",
        "\n",
        "This will be easier to understand if you have read through the logistic regression tutorial."
      ],
      "metadata": {
        "id": "yCCG8kKHCVnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the model\n",
        "class DiabetesModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DiabetesModel, self).__init__()\n",
        "        # we'll set up the layers as a sequence using nn.Sequential\n",
        "        self.layers = nn.Sequential(\n",
        "\n",
        "            # first layer will be a linear layer that has 5x neurons\n",
        "            # (5x sets of linear regression)\n",
        "            # the layer takes the 10 features as input (i.e. 10, 5)\n",
        "            nn.Linear(10, 5),\n",
        "\n",
        "            nn.ReLU(), # ReLU activation\n",
        "\n",
        "            # second linear layer again has 5 neurons\n",
        "            # this time taking the input as the output of the last layer\n",
        "            # (which had 5x neurons)\n",
        "            nn.Linear(5, 5),\n",
        "\n",
        "            nn.ReLU(), # ReLU again\n",
        "\n",
        "            # last linear layer takes the output from the previous 5 neurons\n",
        "            # this time its a single output with no activation\n",
        "            # i.e. this is the predicitons (regression)\n",
        "            nn.Linear(5, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x) # pass the data through the layers"
      ],
      "metadata": {
        "id": "844H60hcCV3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before we need to create a model object, specify the loss (criterion) and an optimiser (which we cover next week):"
      ],
      "metadata": {
        "id": "cv4-loCz91aa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = DiabetesModel()\n",
        "criterion = nn.MSELoss() # MSE loss function\n",
        "optimiser = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "EPx_Wy6g9uA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can train the model. Again, the logistic regression tutorial (6.01) may help you undertstand this:"
      ],
      "metadata": {
        "id": "HOKfjkfW-Ish"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Training loop (example - you'll likely want to add more epochs)\n",
        "epochs = 100 # 100 epochs\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  # use the train_loader to pass the inputs (x) and targets (y)\n",
        "  for inputs, targets in train_loader:\n",
        "    # pass to the GPU (hopefully)\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "    # pass model to GPU as well\n",
        "    model.to(device)\n",
        "\n",
        "    model.train() # put the model object in train mode\n",
        "    optimiser.zero_grad() # reset the gradiants\n",
        "    outputs = model(inputs) # create outputs\n",
        "    loss = criterion(outputs, targets) # compare with Y to get loss\n",
        "    loss.backward() # backpropogate the loss (next week)\n",
        "    optimiser.step() # # update the parameters based on this round of training\n",
        "\n",
        "  # every 10 steps we will print out the current loss\n",
        "    if (epoch+1) % 10 == 0: # modular arithmetic\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {round(loss.item(), 4)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtMUgfwT-HGt",
        "outputId": "a6cb627c-6f2e-4060-e4b2-f3bf9514d25a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 29939.5293\n",
            "Epoch [10/100], Loss: 34498.1914\n",
            "Epoch [10/100], Loss: 29559.957\n",
            "Epoch [10/100], Loss: 24518.5762\n",
            "Epoch [10/100], Loss: 32102.8984\n",
            "Epoch [10/100], Loss: 26721.1387\n",
            "Epoch [10/100], Loss: 29441.6113\n",
            "Epoch [10/100], Loss: 35415.4609\n",
            "Epoch [20/100], Loss: 25080.1172\n",
            "Epoch [20/100], Loss: 29300.5\n",
            "Epoch [20/100], Loss: 28319.2109\n",
            "Epoch [20/100], Loss: 32645.1543\n",
            "Epoch [20/100], Loss: 35287.8125\n",
            "Epoch [20/100], Loss: 29814.3145\n",
            "Epoch [20/100], Loss: 26914.1816\n",
            "Epoch [20/100], Loss: 15273.9951\n",
            "Epoch [30/100], Loss: 25672.8047\n",
            "Epoch [30/100], Loss: 30351.8652\n",
            "Epoch [30/100], Loss: 25934.1445\n",
            "Epoch [30/100], Loss: 34770.8906\n",
            "Epoch [30/100], Loss: 27245.8418\n",
            "Epoch [30/100], Loss: 31053.9043\n",
            "Epoch [30/100], Loss: 31543.4277\n",
            "Epoch [30/100], Loss: 8494.333\n",
            "Epoch [40/100], Loss: 24847.6797\n",
            "Epoch [40/100], Loss: 25686.2637\n",
            "Epoch [40/100], Loss: 30448.7949\n",
            "Epoch [40/100], Loss: 35888.1445\n",
            "Epoch [40/100], Loss: 31023.6387\n",
            "Epoch [40/100], Loss: 27028.666\n",
            "Epoch [40/100], Loss: 28550.9277\n",
            "Epoch [40/100], Loss: 21457.5996\n",
            "Epoch [50/100], Loss: 25344.4551\n",
            "Epoch [50/100], Loss: 25410.9062\n",
            "Epoch [50/100], Loss: 32141.3535\n",
            "Epoch [50/100], Loss: 27045.7148\n",
            "Epoch [50/100], Loss: 35638.8711\n",
            "Epoch [50/100], Loss: 25618.334\n",
            "Epoch [50/100], Loss: 28632.0938\n",
            "Epoch [50/100], Loss: 14916.2061\n",
            "Epoch [60/100], Loss: 25538.1699\n",
            "Epoch [60/100], Loss: 37172.7227\n",
            "Epoch [60/100], Loss: 28295.5391\n",
            "Epoch [60/100], Loss: 27851.8574\n",
            "Epoch [60/100], Loss: 30591.7363\n",
            "Epoch [60/100], Loss: 26866.3047\n",
            "Epoch [60/100], Loss: 15462.04\n",
            "Epoch [60/100], Loss: 48969.2461\n",
            "Epoch [70/100], Loss: 26540.5645\n",
            "Epoch [70/100], Loss: 29484.7402\n",
            "Epoch [70/100], Loss: 33954.5703\n",
            "Epoch [70/100], Loss: 22656.6973\n",
            "Epoch [70/100], Loss: 23080.1016\n",
            "Epoch [70/100], Loss: 22390.9004\n",
            "Epoch [70/100], Loss: 25648.5566\n",
            "Epoch [70/100], Loss: 45067.2188\n",
            "Epoch [80/100], Loss: 27894.1328\n",
            "Epoch [80/100], Loss: 20478.748\n",
            "Epoch [80/100], Loss: 23260.9922\n",
            "Epoch [80/100], Loss: 23926.916\n",
            "Epoch [80/100], Loss: 29546.082\n",
            "Epoch [80/100], Loss: 25231.2148\n",
            "Epoch [80/100], Loss: 23134.9238\n",
            "Epoch [80/100], Loss: 39591.9922\n",
            "Epoch [90/100], Loss: 25173.5293\n",
            "Epoch [90/100], Loss: 20459.9609\n",
            "Epoch [90/100], Loss: 22416.0\n",
            "Epoch [90/100], Loss: 21130.7539\n",
            "Epoch [90/100], Loss: 30919.0625\n",
            "Epoch [90/100], Loss: 22925.3223\n",
            "Epoch [90/100], Loss: 18549.0762\n",
            "Epoch [90/100], Loss: 32503.0469\n",
            "Epoch [100/100], Loss: 22227.2852\n",
            "Epoch [100/100], Loss: 24960.584\n",
            "Epoch [100/100], Loss: 19507.3516\n",
            "Epoch [100/100], Loss: 23614.2871\n",
            "Epoch [100/100], Loss: 21300.3242\n",
            "Epoch [100/100], Loss: 18119.2969\n",
            "Epoch [100/100], Loss: 17212.3066\n",
            "Epoch [100/100], Loss: 39953.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see loss is significantly lower at the end than it was at the start. However, it is also bouncing around a little still which suggests the model needs more training (100 epochs is not a lot in deep learning terms). However, let's evaluate as before:"
      ],
      "metadata": {
        "id": "E72ZTKSqAODE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation (example)\n",
        "model.eval() # testing mode\n",
        "mse_values = [] # collect the MSE scores\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs) # predict the test data\n",
        "\n",
        "        # Calculate Mean Squared Error\n",
        "        mse = criterion(outputs, targets) # calcualte mse for the batch\n",
        "        mse_values.append(mse.item()) # add to the list of MSE values\n",
        "\n",
        "# Calculate and print the average MSE\n",
        "avg_mse = np.mean(mse_values)\n",
        "print(f\"Average MSE on test set: {avg_mse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbuAH6p8A-Vh",
        "outputId": "4b25d253-1e62-425c-8809-5fc452e5a7df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average MSE on test set: 18197.8701171875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MSE looks expected given training (no obvious sign of overfitting). However, we probably can get better results with tuning and more epochs.\n",
        "\n",
        "Let's run the loop again a little differently to collect the predicted values (y_hat) and actuals (y) and add them to a dataset for comparions:"
      ],
      "metadata": {
        "id": "HQ26bA08Up12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "model.eval()\n",
        "predictions = []\n",
        "actuals = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        predictions.extend(outputs.cpu().numpy())\n",
        "        actuals.extend(targets.cpu().numpy())\n",
        "\n",
        "# Create DataFrame\n",
        "results_df = pd.DataFrame({'Predicted': np.array(predictions).flatten(), 'Actual': np.array(actuals).flatten()})\n",
        "results_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "8AYsDDSLUp_u",
        "outputId": "8d9030a5-329d-4258-a441-2a3e72d82eb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Predicted  Actual\n",
              "0   32.091373   219.0\n",
              "1   30.123177    70.0\n",
              "2   31.127077   202.0\n",
              "3   39.057858   230.0\n",
              "4   30.854511   111.0\n",
              "..        ...     ...\n",
              "84  27.300570   153.0\n",
              "85  25.217506    98.0\n",
              "86  22.807533    37.0\n",
              "87  24.040789    63.0\n",
              "88  28.316441   184.0\n",
              "\n",
              "[89 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7066c123-53b2-47e6-a560-c366f96c7af1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted</th>\n",
              "      <th>Actual</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>32.091373</td>\n",
              "      <td>219.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>30.123177</td>\n",
              "      <td>70.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>31.127077</td>\n",
              "      <td>202.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>39.057858</td>\n",
              "      <td>230.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>30.854511</td>\n",
              "      <td>111.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>27.300570</td>\n",
              "      <td>153.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>25.217506</td>\n",
              "      <td>98.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>22.807533</td>\n",
              "      <td>37.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>24.040789</td>\n",
              "      <td>63.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>28.316441</td>\n",
              "      <td>184.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>89 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7066c123-53b2-47e6-a560-c366f96c7af1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7066c123-53b2-47e6-a560-c366f96c7af1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7066c123-53b2-47e6-a560-c366f96c7af1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_19bef840-7fbd-4531-aa71-d6c07ae96c7c\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_19bef840-7fbd-4531-aa71-d6c07ae96c7c button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('results_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "results_df",
              "summary": "{\n  \"name\": \"results_df\",\n  \"rows\": 89,\n  \"fields\": [\n    {\n      \"column\": \"Predicted\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 89,\n        \"samples\": [\n          30.557775497436523,\n          27.114139556884766,\n          31.79819107055664\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Actual\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          111.0,\n          61.0,\n          252.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Side-by-side, they don't look great. Can you improve them?\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## EXERCISE #1\n",
        "Try increasing the number of epochs to 1,000 (when the model is fairly well trained then the results printed for each 10x epochs will be fairly stable and not change much). Does this give better results?\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## EXERCISE #2 (optional)\n",
        "Try experimenting with the architecture (number of neurons and/or number of layers). Can we reach an optimal architecture?"
      ],
      "metadata": {
        "id": "LDcM98lHbgP8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7961cb60"
      },
      "source": [
        "**Exercise 1**:\n",
        "Since the task requires increasing the number of epochs to 1000, I will train the data with these new epochs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "423ae062",
        "outputId": "b3301066-531b-4d45-e5bb-285a1484b8a1"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Training loop (example - you'll likely want to add more epochs)\n",
        "epochs = 1000 # 1000 epochs\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  # use the train_loader to pass the inputs (x) and targets (y)\n",
        "  for inputs, targets in train_loader:\n",
        "    # pass to the GPU (hopefully)\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "    # pass model to GPU as well\n",
        "    model.to(device)\n",
        "\n",
        "    model.train() # put the model object in train mode\n",
        "    optimiser.zero_grad() # reset the gradiants\n",
        "    outputs = model(inputs) # create outputs\n",
        "    loss = criterion(outputs, targets) # compare with Y to get loss\n",
        "    loss.backward() # backpropogate the loss (next week)\n",
        "    optimiser.step() # # update the parameters based on this round of training\n",
        "\n",
        "  # every 10 steps we will print out the current loss\n",
        "    if (epoch+1) % 10 == 0: # modular arithmetic\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {round(loss.item(), 4)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/1000], Loss: 16104.7324\n",
            "Epoch [10/1000], Loss: 20242.7051\n",
            "Epoch [10/1000], Loss: 17160.3438\n",
            "Epoch [10/1000], Loss: 16775.6895\n",
            "Epoch [10/1000], Loss: 17958.252\n",
            "Epoch [10/1000], Loss: 23172.7695\n",
            "Epoch [10/1000], Loss: 20607.6719\n",
            "Epoch [10/1000], Loss: 31545.3965\n",
            "Epoch [20/1000], Loss: 15590.8271\n",
            "Epoch [20/1000], Loss: 17692.3066\n",
            "Epoch [20/1000], Loss: 21618.709\n",
            "Epoch [20/1000], Loss: 11813.832\n",
            "Epoch [20/1000], Loss: 16399.3066\n",
            "Epoch [20/1000], Loss: 18102.2266\n",
            "Epoch [20/1000], Loss: 15564.9961\n",
            "Epoch [20/1000], Loss: 17341.3828\n",
            "Epoch [30/1000], Loss: 11873.2822\n",
            "Epoch [30/1000], Loss: 16085.75\n",
            "Epoch [30/1000], Loss: 12025.6006\n",
            "Epoch [30/1000], Loss: 19631.332\n",
            "Epoch [30/1000], Loss: 15451.8096\n",
            "Epoch [30/1000], Loss: 15782.6172\n",
            "Epoch [30/1000], Loss: 10762.9248\n",
            "Epoch [30/1000], Loss: 4040.8943\n",
            "Epoch [40/1000], Loss: 10954.4658\n",
            "Epoch [40/1000], Loss: 11057.5723\n",
            "Epoch [40/1000], Loss: 10706.8184\n",
            "Epoch [40/1000], Loss: 8409.6553\n",
            "Epoch [40/1000], Loss: 16843.3164\n",
            "Epoch [40/1000], Loss: 11844.54\n",
            "Epoch [40/1000], Loss: 16361.8311\n",
            "Epoch [40/1000], Loss: 10881.9014\n",
            "Epoch [50/1000], Loss: 13383.1123\n",
            "Epoch [50/1000], Loss: 9505.6016\n",
            "Epoch [50/1000], Loss: 10085.0762\n",
            "Epoch [50/1000], Loss: 12043.8223\n",
            "Epoch [50/1000], Loss: 10099.4102\n",
            "Epoch [50/1000], Loss: 9669.8076\n",
            "Epoch [50/1000], Loss: 7232.6484\n",
            "Epoch [50/1000], Loss: 16925.5352\n",
            "Epoch [60/1000], Loss: 7404.6724\n",
            "Epoch [60/1000], Loss: 6573.7305\n",
            "Epoch [60/1000], Loss: 9459.1348\n",
            "Epoch [60/1000], Loss: 8176.9565\n",
            "Epoch [60/1000], Loss: 13626.4648\n",
            "Epoch [60/1000], Loss: 8444.2432\n",
            "Epoch [60/1000], Loss: 7036.3486\n",
            "Epoch [60/1000], Loss: 4719.9253\n",
            "Epoch [70/1000], Loss: 5251.1626\n",
            "Epoch [70/1000], Loss: 5670.4336\n",
            "Epoch [70/1000], Loss: 8079.4609\n",
            "Epoch [70/1000], Loss: 8253.167\n",
            "Epoch [70/1000], Loss: 7797.7446\n",
            "Epoch [70/1000], Loss: 10453.7822\n",
            "Epoch [70/1000], Loss: 5703.4199\n",
            "Epoch [70/1000], Loss: 3643.3018\n",
            "Epoch [80/1000], Loss: 5621.6157\n",
            "Epoch [80/1000], Loss: 8958.4883\n",
            "Epoch [80/1000], Loss: 4562.9453\n",
            "Epoch [80/1000], Loss: 5612.8013\n",
            "Epoch [80/1000], Loss: 5955.7173\n",
            "Epoch [80/1000], Loss: 6846.8535\n",
            "Epoch [80/1000], Loss: 5810.5259\n",
            "Epoch [80/1000], Loss: 11263.3242\n",
            "Epoch [90/1000], Loss: 4431.5503\n",
            "Epoch [90/1000], Loss: 7003.8979\n",
            "Epoch [90/1000], Loss: 4190.7617\n",
            "Epoch [90/1000], Loss: 5045.0342\n",
            "Epoch [90/1000], Loss: 5346.6255\n",
            "Epoch [90/1000], Loss: 5575.0376\n",
            "Epoch [90/1000], Loss: 6853.0737\n",
            "Epoch [90/1000], Loss: 1786.9359\n",
            "Epoch [100/1000], Loss: 5729.5566\n",
            "Epoch [100/1000], Loss: 4274.1348\n",
            "Epoch [100/1000], Loss: 5079.7505\n",
            "Epoch [100/1000], Loss: 4274.8716\n",
            "Epoch [100/1000], Loss: 3622.0\n",
            "Epoch [100/1000], Loss: 6140.8442\n",
            "Epoch [100/1000], Loss: 5654.3887\n",
            "Epoch [100/1000], Loss: 2798.5505\n",
            "Epoch [110/1000], Loss: 4509.5938\n",
            "Epoch [110/1000], Loss: 4689.2344\n",
            "Epoch [110/1000], Loss: 4229.9409\n",
            "Epoch [110/1000], Loss: 4144.0215\n",
            "Epoch [110/1000], Loss: 4718.8042\n",
            "Epoch [110/1000], Loss: 5934.6562\n",
            "Epoch [110/1000], Loss: 4127.4326\n",
            "Epoch [110/1000], Loss: 3050.1973\n",
            "Epoch [120/1000], Loss: 3602.4253\n",
            "Epoch [120/1000], Loss: 4085.3594\n",
            "Epoch [120/1000], Loss: 4319.5889\n",
            "Epoch [120/1000], Loss: 4531.8271\n",
            "Epoch [120/1000], Loss: 3948.9788\n",
            "Epoch [120/1000], Loss: 4707.2026\n",
            "Epoch [120/1000], Loss: 5109.6909\n",
            "Epoch [120/1000], Loss: 10304.502\n",
            "Epoch [130/1000], Loss: 4379.0635\n",
            "Epoch [130/1000], Loss: 4917.457\n",
            "Epoch [130/1000], Loss: 4155.4551\n",
            "Epoch [130/1000], Loss: 3666.3005\n",
            "Epoch [130/1000], Loss: 4157.29\n",
            "Epoch [130/1000], Loss: 3821.8176\n",
            "Epoch [130/1000], Loss: 4833.1426\n",
            "Epoch [130/1000], Loss: 1288.0728\n",
            "Epoch [140/1000], Loss: 3871.7856\n",
            "Epoch [140/1000], Loss: 4240.5479\n",
            "Epoch [140/1000], Loss: 4752.8901\n",
            "Epoch [140/1000], Loss: 4232.2153\n",
            "Epoch [140/1000], Loss: 4258.1548\n",
            "Epoch [140/1000], Loss: 4444.4673\n",
            "Epoch [140/1000], Loss: 3284.7217\n",
            "Epoch [140/1000], Loss: 5600.9336\n",
            "Epoch [150/1000], Loss: 5560.2979\n",
            "Epoch [150/1000], Loss: 3413.4143\n",
            "Epoch [150/1000], Loss: 3579.708\n",
            "Epoch [150/1000], Loss: 4715.4097\n",
            "Epoch [150/1000], Loss: 3153.0203\n",
            "Epoch [150/1000], Loss: 4461.147\n",
            "Epoch [150/1000], Loss: 3922.4189\n",
            "Epoch [150/1000], Loss: 3092.0391\n",
            "Epoch [160/1000], Loss: 3634.5759\n",
            "Epoch [160/1000], Loss: 4471.292\n",
            "Epoch [160/1000], Loss: 4033.0308\n",
            "Epoch [160/1000], Loss: 3568.6182\n",
            "Epoch [160/1000], Loss: 4827.1406\n",
            "Epoch [160/1000], Loss: 3282.2222\n",
            "Epoch [160/1000], Loss: 4627.6201\n",
            "Epoch [160/1000], Loss: 3550.8716\n",
            "Epoch [170/1000], Loss: 5147.3315\n",
            "Epoch [170/1000], Loss: 4031.1479\n",
            "Epoch [170/1000], Loss: 3793.9062\n",
            "Epoch [170/1000], Loss: 3393.1943\n",
            "Epoch [170/1000], Loss: 4525.356\n",
            "Epoch [170/1000], Loss: 3935.8762\n",
            "Epoch [170/1000], Loss: 3409.4185\n",
            "Epoch [170/1000], Loss: 1971.3723\n",
            "Epoch [180/1000], Loss: 4476.9673\n",
            "Epoch [180/1000], Loss: 3745.408\n",
            "Epoch [180/1000], Loss: 3064.999\n",
            "Epoch [180/1000], Loss: 3976.3186\n",
            "Epoch [180/1000], Loss: 4368.9258\n",
            "Epoch [180/1000], Loss: 3916.9167\n",
            "Epoch [180/1000], Loss: 4162.5107\n",
            "Epoch [180/1000], Loss: 6492.9131\n",
            "Epoch [190/1000], Loss: 3326.4055\n",
            "Epoch [190/1000], Loss: 4099.417\n",
            "Epoch [190/1000], Loss: 3853.7424\n",
            "Epoch [190/1000], Loss: 3342.8687\n",
            "Epoch [190/1000], Loss: 3681.0171\n",
            "Epoch [190/1000], Loss: 4336.5845\n",
            "Epoch [190/1000], Loss: 4926.7925\n",
            "Epoch [190/1000], Loss: 4569.1392\n",
            "Epoch [200/1000], Loss: 3827.2122\n",
            "Epoch [200/1000], Loss: 4117.7295\n",
            "Epoch [200/1000], Loss: 2987.363\n",
            "Epoch [200/1000], Loss: 3383.8879\n",
            "Epoch [200/1000], Loss: 5215.6025\n",
            "Epoch [200/1000], Loss: 4061.8438\n",
            "Epoch [200/1000], Loss: 3823.8562\n",
            "Epoch [200/1000], Loss: 3367.6382\n",
            "Epoch [210/1000], Loss: 2984.0469\n",
            "Epoch [210/1000], Loss: 3389.9426\n",
            "Epoch [210/1000], Loss: 3397.2666\n",
            "Epoch [210/1000], Loss: 5087.0098\n",
            "Epoch [210/1000], Loss: 5010.6396\n",
            "Epoch [210/1000], Loss: 3954.5862\n",
            "Epoch [210/1000], Loss: 3263.3503\n",
            "Epoch [210/1000], Loss: 4888.7852\n",
            "Epoch [220/1000], Loss: 3503.5393\n",
            "Epoch [220/1000], Loss: 3858.1648\n",
            "Epoch [220/1000], Loss: 3130.2231\n",
            "Epoch [220/1000], Loss: 4733.2129\n",
            "Epoch [220/1000], Loss: 4451.8525\n",
            "Epoch [220/1000], Loss: 3672.8538\n",
            "Epoch [220/1000], Loss: 3678.0906\n",
            "Epoch [220/1000], Loss: 1808.5564\n",
            "Epoch [230/1000], Loss: 4911.6641\n",
            "Epoch [230/1000], Loss: 3410.603\n",
            "Epoch [230/1000], Loss: 3694.0203\n",
            "Epoch [230/1000], Loss: 4295.3276\n",
            "Epoch [230/1000], Loss: 3481.4556\n",
            "Epoch [230/1000], Loss: 3174.6067\n",
            "Epoch [230/1000], Loss: 3838.312\n",
            "Epoch [230/1000], Loss: 1973.2889\n",
            "Epoch [240/1000], Loss: 3885.7778\n",
            "Epoch [240/1000], Loss: 3464.4167\n",
            "Epoch [240/1000], Loss: 2906.1487\n",
            "Epoch [240/1000], Loss: 4805.2798\n",
            "Epoch [240/1000], Loss: 3961.2681\n",
            "Epoch [240/1000], Loss: 3974.6899\n",
            "Epoch [240/1000], Loss: 3207.8047\n",
            "Epoch [240/1000], Loss: 8812.0488\n",
            "Epoch [250/1000], Loss: 3563.8796\n",
            "Epoch [250/1000], Loss: 4199.1509\n",
            "Epoch [250/1000], Loss: 3707.2925\n",
            "Epoch [250/1000], Loss: 4864.0454\n",
            "Epoch [250/1000], Loss: 2468.0505\n",
            "Epoch [250/1000], Loss: 3877.2683\n",
            "Epoch [250/1000], Loss: 3667.3406\n",
            "Epoch [250/1000], Loss: 3125.8035\n",
            "Epoch [260/1000], Loss: 4180.3228\n",
            "Epoch [260/1000], Loss: 3406.7983\n",
            "Epoch [260/1000], Loss: 2909.9163\n",
            "Epoch [260/1000], Loss: 3880.8406\n",
            "Epoch [260/1000], Loss: 3503.0181\n",
            "Epoch [260/1000], Loss: 4394.9438\n",
            "Epoch [260/1000], Loss: 3913.8286\n",
            "Epoch [260/1000], Loss: 2933.7793\n",
            "Epoch [270/1000], Loss: 4994.8198\n",
            "Epoch [270/1000], Loss: 3829.071\n",
            "Epoch [270/1000], Loss: 3619.1143\n",
            "Epoch [270/1000], Loss: 3318.5974\n",
            "Epoch [270/1000], Loss: 3758.7629\n",
            "Epoch [270/1000], Loss: 2815.2937\n",
            "Epoch [270/1000], Loss: 3798.0686\n",
            "Epoch [270/1000], Loss: 655.205\n",
            "Epoch [280/1000], Loss: 3407.03\n",
            "Epoch [280/1000], Loss: 4038.5591\n",
            "Epoch [280/1000], Loss: 3685.6418\n",
            "Epoch [280/1000], Loss: 3674.4321\n",
            "Epoch [280/1000], Loss: 3362.1138\n",
            "Epoch [280/1000], Loss: 3117.9836\n",
            "Epoch [280/1000], Loss: 4290.6538\n",
            "Epoch [280/1000], Loss: 7064.165\n",
            "Epoch [290/1000], Loss: 3883.0662\n",
            "Epoch [290/1000], Loss: 3316.4944\n",
            "Epoch [290/1000], Loss: 2896.3889\n",
            "Epoch [290/1000], Loss: 3965.7114\n",
            "Epoch [290/1000], Loss: 4393.686\n",
            "Epoch [290/1000], Loss: 3479.0498\n",
            "Epoch [290/1000], Loss: 3572.3606\n",
            "Epoch [290/1000], Loss: 5201.877\n",
            "Epoch [300/1000], Loss: 2769.8831\n",
            "Epoch [300/1000], Loss: 4291.6772\n",
            "Epoch [300/1000], Loss: 2548.1621\n",
            "Epoch [300/1000], Loss: 3541.7483\n",
            "Epoch [300/1000], Loss: 3920.2351\n",
            "Epoch [300/1000], Loss: 3088.7808\n",
            "Epoch [300/1000], Loss: 5109.8086\n",
            "Epoch [300/1000], Loss: 6266.6558\n",
            "Epoch [310/1000], Loss: 4147.0884\n",
            "Epoch [310/1000], Loss: 2370.7722\n",
            "Epoch [310/1000], Loss: 3762.3977\n",
            "Epoch [310/1000], Loss: 3603.9111\n",
            "Epoch [310/1000], Loss: 3729.0217\n",
            "Epoch [310/1000], Loss: 3611.5513\n",
            "Epoch [310/1000], Loss: 3869.0977\n",
            "Epoch [310/1000], Loss: 6306.5938\n",
            "Epoch [320/1000], Loss: 5166.2065\n",
            "Epoch [320/1000], Loss: 3287.9912\n",
            "Epoch [320/1000], Loss: 2370.9277\n",
            "Epoch [320/1000], Loss: 3851.0493\n",
            "Epoch [320/1000], Loss: 3286.0212\n",
            "Epoch [320/1000], Loss: 2858.8354\n",
            "Epoch [320/1000], Loss: 4311.6958\n",
            "Epoch [320/1000], Loss: 2811.4331\n",
            "Epoch [330/1000], Loss: 3782.6021\n",
            "Epoch [330/1000], Loss: 3801.23\n",
            "Epoch [330/1000], Loss: 3724.7585\n",
            "Epoch [330/1000], Loss: 3008.739\n",
            "Epoch [330/1000], Loss: 3359.7549\n",
            "Epoch [330/1000], Loss: 4103.7676\n",
            "Epoch [330/1000], Loss: 3222.4033\n",
            "Epoch [330/1000], Loss: 2383.9172\n",
            "Epoch [340/1000], Loss: 3365.0737\n",
            "Epoch [340/1000], Loss: 3189.5994\n",
            "Epoch [340/1000], Loss: 3130.1467\n",
            "Epoch [340/1000], Loss: 3486.0051\n",
            "Epoch [340/1000], Loss: 4546.0967\n",
            "Epoch [340/1000], Loss: 3763.2402\n",
            "Epoch [340/1000], Loss: 3450.4199\n",
            "Epoch [340/1000], Loss: 1041.5782\n",
            "Epoch [350/1000], Loss: 3547.3086\n",
            "Epoch [350/1000], Loss: 3282.5337\n",
            "Epoch [350/1000], Loss: 3156.0261\n",
            "Epoch [350/1000], Loss: 3660.8875\n",
            "Epoch [350/1000], Loss: 4340.9082\n",
            "Epoch [350/1000], Loss: 3674.854\n",
            "Epoch [350/1000], Loss: 2993.6985\n",
            "Epoch [350/1000], Loss: 3088.6602\n",
            "Epoch [360/1000], Loss: 2973.9072\n",
            "Epoch [360/1000], Loss: 3205.8962\n",
            "Epoch [360/1000], Loss: 4517.4092\n",
            "Epoch [360/1000], Loss: 3234.1667\n",
            "Epoch [360/1000], Loss: 3275.519\n",
            "Epoch [360/1000], Loss: 3177.5938\n",
            "Epoch [360/1000], Loss: 4201.8237\n",
            "Epoch [360/1000], Loss: 2028.8544\n",
            "Epoch [370/1000], Loss: 3940.4773\n",
            "Epoch [370/1000], Loss: 4066.3806\n",
            "Epoch [370/1000], Loss: 4010.543\n",
            "Epoch [370/1000], Loss: 3172.1699\n",
            "Epoch [370/1000], Loss: 2566.24\n",
            "Epoch [370/1000], Loss: 2767.0156\n",
            "Epoch [370/1000], Loss: 4014.5708\n",
            "Epoch [370/1000], Loss: 28.8037\n",
            "Epoch [380/1000], Loss: 2824.9868\n",
            "Epoch [380/1000], Loss: 2730.2812\n",
            "Epoch [380/1000], Loss: 3371.2036\n",
            "Epoch [380/1000], Loss: 5252.1919\n",
            "Epoch [380/1000], Loss: 3291.0613\n",
            "Epoch [380/1000], Loss: 3457.4639\n",
            "Epoch [380/1000], Loss: 3353.4307\n",
            "Epoch [380/1000], Loss: 2187.6104\n",
            "Epoch [390/1000], Loss: 3533.751\n",
            "Epoch [390/1000], Loss: 3526.7605\n",
            "Epoch [390/1000], Loss: 4518.0137\n",
            "Epoch [390/1000], Loss: 3253.7156\n",
            "Epoch [390/1000], Loss: 2890.1721\n",
            "Epoch [390/1000], Loss: 2846.731\n",
            "Epoch [390/1000], Loss: 3095.1086\n",
            "Epoch [390/1000], Loss: 10143.4453\n",
            "Epoch [400/1000], Loss: 2943.8313\n",
            "Epoch [400/1000], Loss: 4034.9275\n",
            "Epoch [400/1000], Loss: 4074.1003\n",
            "Epoch [400/1000], Loss: 3073.0046\n",
            "Epoch [400/1000], Loss: 4013.3389\n",
            "Epoch [400/1000], Loss: 2151.448\n",
            "Epoch [400/1000], Loss: 3646.2043\n",
            "Epoch [400/1000], Loss: 3267.897\n",
            "Epoch [410/1000], Loss: 3793.2734\n",
            "Epoch [410/1000], Loss: 2541.2751\n",
            "Epoch [410/1000], Loss: 3768.0928\n",
            "Epoch [410/1000], Loss: 3824.158\n",
            "Epoch [410/1000], Loss: 2773.5408\n",
            "Epoch [410/1000], Loss: 3977.3455\n",
            "Epoch [410/1000], Loss: 2999.4753\n",
            "Epoch [410/1000], Loss: 5491.0854\n",
            "Epoch [420/1000], Loss: 3870.2437\n",
            "Epoch [420/1000], Loss: 3552.0625\n",
            "Epoch [420/1000], Loss: 3279.6943\n",
            "Epoch [420/1000], Loss: 3149.4453\n",
            "Epoch [420/1000], Loss: 2930.0374\n",
            "Epoch [420/1000], Loss: 3594.1306\n",
            "Epoch [420/1000], Loss: 3304.3003\n",
            "Epoch [420/1000], Loss: 3457.3853\n",
            "Epoch [430/1000], Loss: 2501.5957\n",
            "Epoch [430/1000], Loss: 4282.9253\n",
            "Epoch [430/1000], Loss: 3697.2256\n",
            "Epoch [430/1000], Loss: 2696.6074\n",
            "Epoch [430/1000], Loss: 3815.9502\n",
            "Epoch [430/1000], Loss: 3275.126\n",
            "Epoch [430/1000], Loss: 3407.1956\n",
            "Epoch [430/1000], Loss: 1155.6927\n",
            "Epoch [440/1000], Loss: 3164.3374\n",
            "Epoch [440/1000], Loss: 3217.8745\n",
            "Epoch [440/1000], Loss: 3679.6074\n",
            "Epoch [440/1000], Loss: 3846.0037\n",
            "Epoch [440/1000], Loss: 2781.2646\n",
            "Epoch [440/1000], Loss: 3513.1455\n",
            "Epoch [440/1000], Loss: 2528.9133\n",
            "Epoch [440/1000], Loss: 14921.1201\n",
            "Epoch [450/1000], Loss: 2432.3516\n",
            "Epoch [450/1000], Loss: 2581.5774\n",
            "Epoch [450/1000], Loss: 3626.0784\n",
            "Epoch [450/1000], Loss: 3407.4988\n",
            "Epoch [450/1000], Loss: 3339.7778\n",
            "Epoch [450/1000], Loss: 4580.9951\n",
            "Epoch [450/1000], Loss: 3252.9722\n",
            "Epoch [450/1000], Loss: 4792.3442\n",
            "Epoch [460/1000], Loss: 3570.5415\n",
            "Epoch [460/1000], Loss: 2940.8015\n",
            "Epoch [460/1000], Loss: 3144.3975\n",
            "Epoch [460/1000], Loss: 3508.9902\n",
            "Epoch [460/1000], Loss: 3841.5586\n",
            "Epoch [460/1000], Loss: 2867.5305\n",
            "Epoch [460/1000], Loss: 3089.8254\n",
            "Epoch [460/1000], Loss: 7315.1084\n",
            "Epoch [470/1000], Loss: 3248.3274\n",
            "Epoch [470/1000], Loss: 3289.3438\n",
            "Epoch [470/1000], Loss: 3446.3181\n",
            "Epoch [470/1000], Loss: 3524.6948\n",
            "Epoch [470/1000], Loss: 3035.729\n",
            "Epoch [470/1000], Loss: 2988.5942\n",
            "Epoch [470/1000], Loss: 3506.5469\n",
            "Epoch [470/1000], Loss: 4324.4922\n",
            "Epoch [480/1000], Loss: 3316.2681\n",
            "Epoch [480/1000], Loss: 4180.7886\n",
            "Epoch [480/1000], Loss: 2760.064\n",
            "Epoch [480/1000], Loss: 3330.6599\n",
            "Epoch [480/1000], Loss: 3416.7996\n",
            "Epoch [480/1000], Loss: 3003.9443\n",
            "Epoch [480/1000], Loss: 3059.2261\n",
            "Epoch [480/1000], Loss: 2129.895\n",
            "Epoch [490/1000], Loss: 3889.2395\n",
            "Epoch [490/1000], Loss: 2765.9485\n",
            "Epoch [490/1000], Loss: 3113.2993\n",
            "Epoch [490/1000], Loss: 2347.928\n",
            "Epoch [490/1000], Loss: 3402.0149\n",
            "Epoch [490/1000], Loss: 3700.4275\n",
            "Epoch [490/1000], Loss: 3566.4143\n",
            "Epoch [490/1000], Loss: 5320.019\n",
            "Epoch [500/1000], Loss: 3359.4285\n",
            "Epoch [500/1000], Loss: 3213.5774\n",
            "Epoch [500/1000], Loss: 3757.6636\n",
            "Epoch [500/1000], Loss: 2713.145\n",
            "Epoch [500/1000], Loss: 2471.3384\n",
            "Epoch [500/1000], Loss: 4007.448\n",
            "Epoch [500/1000], Loss: 3391.9236\n",
            "Epoch [500/1000], Loss: 1625.093\n",
            "Epoch [510/1000], Loss: 4536.9873\n",
            "Epoch [510/1000], Loss: 2857.158\n",
            "Epoch [510/1000], Loss: 2341.082\n",
            "Epoch [510/1000], Loss: 3682.7893\n",
            "Epoch [510/1000], Loss: 3476.6536\n",
            "Epoch [510/1000], Loss: 2899.1143\n",
            "Epoch [510/1000], Loss: 2896.7422\n",
            "Epoch [510/1000], Loss: 3665.9595\n",
            "Epoch [520/1000], Loss: 3882.7839\n",
            "Epoch [520/1000], Loss: 3029.8318\n",
            "Epoch [520/1000], Loss: 3406.1982\n",
            "Epoch [520/1000], Loss: 3050.3359\n",
            "Epoch [520/1000], Loss: 3189.7334\n",
            "Epoch [520/1000], Loss: 2660.7163\n",
            "Epoch [520/1000], Loss: 3319.6011\n",
            "Epoch [520/1000], Loss: 4740.2217\n",
            "Epoch [530/1000], Loss: 2889.5684\n",
            "Epoch [530/1000], Loss: 2958.5305\n",
            "Epoch [530/1000], Loss: 4501.3784\n",
            "Epoch [530/1000], Loss: 3516.2952\n",
            "Epoch [530/1000], Loss: 3713.1858\n",
            "Epoch [530/1000], Loss: 2454.2192\n",
            "Epoch [530/1000], Loss: 2562.4482\n",
            "Epoch [530/1000], Loss: 2408.8774\n",
            "Epoch [540/1000], Loss: 3477.4412\n",
            "Epoch [540/1000], Loss: 3799.095\n",
            "Epoch [540/1000], Loss: 2931.3547\n",
            "Epoch [540/1000], Loss: 2837.1311\n",
            "Epoch [540/1000], Loss: 2445.5425\n",
            "Epoch [540/1000], Loss: 3420.0505\n",
            "Epoch [540/1000], Loss: 3650.3599\n",
            "Epoch [540/1000], Loss: 1324.5479\n",
            "Epoch [550/1000], Loss: 3390.3711\n",
            "Epoch [550/1000], Loss: 3513.6611\n",
            "Epoch [550/1000], Loss: 3910.572\n",
            "Epoch [550/1000], Loss: 2562.9602\n",
            "Epoch [550/1000], Loss: 2920.9163\n",
            "Epoch [550/1000], Loss: 3509.8394\n",
            "Epoch [550/1000], Loss: 2619.0728\n",
            "Epoch [550/1000], Loss: 2344.5007\n",
            "Epoch [560/1000], Loss: 3754.2048\n",
            "Epoch [560/1000], Loss: 3041.0686\n",
            "Epoch [560/1000], Loss: 3095.9187\n",
            "Epoch [560/1000], Loss: 4004.5549\n",
            "Epoch [560/1000], Loss: 2583.3137\n",
            "Epoch [560/1000], Loss: 3495.1406\n",
            "Epoch [560/1000], Loss: 2240.8176\n",
            "Epoch [560/1000], Loss: 4421.1152\n",
            "Epoch [570/1000], Loss: 4131.5869\n",
            "Epoch [570/1000], Loss: 2958.2793\n",
            "Epoch [570/1000], Loss: 2634.6375\n",
            "Epoch [570/1000], Loss: 3372.4983\n",
            "Epoch [570/1000], Loss: 3446.7949\n",
            "Epoch [570/1000], Loss: 2290.55\n",
            "Epoch [570/1000], Loss: 3450.4761\n",
            "Epoch [570/1000], Loss: 2047.1133\n",
            "Epoch [580/1000], Loss: 3385.9231\n",
            "Epoch [580/1000], Loss: 3247.6868\n",
            "Epoch [580/1000], Loss: 2954.0713\n",
            "Epoch [580/1000], Loss: 3111.855\n",
            "Epoch [580/1000], Loss: 3175.3718\n",
            "Epoch [580/1000], Loss: 3235.5356\n",
            "Epoch [580/1000], Loss: 2912.583\n",
            "Epoch [580/1000], Loss: 5332.5991\n",
            "Epoch [590/1000], Loss: 3063.4021\n",
            "Epoch [590/1000], Loss: 2913.4661\n",
            "Epoch [590/1000], Loss: 3215.6899\n",
            "Epoch [590/1000], Loss: 3224.8745\n",
            "Epoch [590/1000], Loss: 3024.5579\n",
            "Epoch [590/1000], Loss: 3685.5127\n",
            "Epoch [590/1000], Loss: 3082.2239\n",
            "Epoch [590/1000], Loss: 1161.3496\n",
            "Epoch [600/1000], Loss: 2184.5312\n",
            "Epoch [600/1000], Loss: 2333.2019\n",
            "Epoch [600/1000], Loss: 3253.9749\n",
            "Epoch [600/1000], Loss: 3622.2332\n",
            "Epoch [600/1000], Loss: 3732.2258\n",
            "Epoch [600/1000], Loss: 3686.4612\n",
            "Epoch [600/1000], Loss: 3216.8274\n",
            "Epoch [600/1000], Loss: 3009.9116\n",
            "Epoch [610/1000], Loss: 2826.1372\n",
            "Epoch [610/1000], Loss: 3555.9744\n",
            "Epoch [610/1000], Loss: 3522.9456\n",
            "Epoch [610/1000], Loss: 3052.7605\n",
            "Epoch [610/1000], Loss: 2174.0251\n",
            "Epoch [610/1000], Loss: 3151.0181\n",
            "Epoch [610/1000], Loss: 3685.5706\n",
            "Epoch [610/1000], Loss: 3085.3687\n",
            "Epoch [620/1000], Loss: 3364.5583\n",
            "Epoch [620/1000], Loss: 3329.7349\n",
            "Epoch [620/1000], Loss: 3048.7434\n",
            "Epoch [620/1000], Loss: 3751.9688\n",
            "Epoch [620/1000], Loss: 3881.7585\n",
            "Epoch [620/1000], Loss: 2470.4285\n",
            "Epoch [620/1000], Loss: 2140.2961\n",
            "Epoch [620/1000], Loss: 1617.5037\n",
            "Epoch [630/1000], Loss: 3084.7092\n",
            "Epoch [630/1000], Loss: 3066.1643\n",
            "Epoch [630/1000], Loss: 3248.8469\n",
            "Epoch [630/1000], Loss: 1988.3136\n",
            "Epoch [630/1000], Loss: 2787.3042\n",
            "Epoch [630/1000], Loss: 3906.2273\n",
            "Epoch [630/1000], Loss: 3673.6431\n",
            "Epoch [630/1000], Loss: 4364.4292\n",
            "Epoch [640/1000], Loss: 2381.1577\n",
            "Epoch [640/1000], Loss: 2808.6746\n",
            "Epoch [640/1000], Loss: 3089.4868\n",
            "Epoch [640/1000], Loss: 2231.7961\n",
            "Epoch [640/1000], Loss: 3556.293\n",
            "Epoch [640/1000], Loss: 4273.2363\n",
            "Epoch [640/1000], Loss: 3341.8013\n",
            "Epoch [640/1000], Loss: 4686.5576\n",
            "Epoch [650/1000], Loss: 2265.9678\n",
            "Epoch [650/1000], Loss: 3323.1199\n",
            "Epoch [650/1000], Loss: 3231.948\n",
            "Epoch [650/1000], Loss: 3734.6343\n",
            "Epoch [650/1000], Loss: 3271.511\n",
            "Epoch [650/1000], Loss: 3333.7981\n",
            "Epoch [650/1000], Loss: 2602.6641\n",
            "Epoch [650/1000], Loss: 2424.8643\n",
            "Epoch [660/1000], Loss: 3828.5581\n",
            "Epoch [660/1000], Loss: 3120.7917\n",
            "Epoch [660/1000], Loss: 3654.2388\n",
            "Epoch [660/1000], Loss: 3215.833\n",
            "Epoch [660/1000], Loss: 3098.1438\n",
            "Epoch [660/1000], Loss: 2273.2693\n",
            "Epoch [660/1000], Loss: 2532.9736\n",
            "Epoch [660/1000], Loss: 2258.4275\n",
            "Epoch [670/1000], Loss: 3196.7637\n",
            "Epoch [670/1000], Loss: 2665.1865\n",
            "Epoch [670/1000], Loss: 3082.6724\n",
            "Epoch [670/1000], Loss: 3053.3013\n",
            "Epoch [670/1000], Loss: 3739.7322\n",
            "Epoch [670/1000], Loss: 3455.5793\n",
            "Epoch [670/1000], Loss: 2536.0513\n",
            "Epoch [670/1000], Loss: 1044.4087\n",
            "Epoch [680/1000], Loss: 3075.4775\n",
            "Epoch [680/1000], Loss: 3756.5669\n",
            "Epoch [680/1000], Loss: 2868.491\n",
            "Epoch [680/1000], Loss: 3095.0818\n",
            "Epoch [680/1000], Loss: 3256.3931\n",
            "Epoch [680/1000], Loss: 2655.2234\n",
            "Epoch [680/1000], Loss: 2829.2903\n",
            "Epoch [680/1000], Loss: 3205.3223\n",
            "Epoch [690/1000], Loss: 4271.6392\n",
            "Epoch [690/1000], Loss: 2030.5725\n",
            "Epoch [690/1000], Loss: 3591.1553\n",
            "Epoch [690/1000], Loss: 2765.9976\n",
            "Epoch [690/1000], Loss: 2541.7739\n",
            "Epoch [690/1000], Loss: 3006.1375\n",
            "Epoch [690/1000], Loss: 3251.7581\n",
            "Epoch [690/1000], Loss: 3632.0225\n",
            "Epoch [700/1000], Loss: 3970.4604\n",
            "Epoch [700/1000], Loss: 3451.6536\n",
            "Epoch [700/1000], Loss: 2483.103\n",
            "Epoch [700/1000], Loss: 2558.4109\n",
            "Epoch [700/1000], Loss: 3296.3003\n",
            "Epoch [700/1000], Loss: 2534.5308\n",
            "Epoch [700/1000], Loss: 3282.1543\n",
            "Epoch [700/1000], Loss: 993.1654\n",
            "Epoch [710/1000], Loss: 2935.0125\n",
            "Epoch [710/1000], Loss: 3351.3318\n",
            "Epoch [710/1000], Loss: 2936.4099\n",
            "Epoch [710/1000], Loss: 3695.7512\n",
            "Epoch [710/1000], Loss: 3455.3103\n",
            "Epoch [710/1000], Loss: 2541.9602\n",
            "Epoch [710/1000], Loss: 2461.0337\n",
            "Epoch [710/1000], Loss: 3358.5876\n",
            "Epoch [720/1000], Loss: 3130.4065\n",
            "Epoch [720/1000], Loss: 2952.1506\n",
            "Epoch [720/1000], Loss: 3144.2378\n",
            "Epoch [720/1000], Loss: 2575.3855\n",
            "Epoch [720/1000], Loss: 2738.3142\n",
            "Epoch [720/1000], Loss: 3283.7617\n",
            "Epoch [720/1000], Loss: 3608.082\n",
            "Epoch [720/1000], Loss: 1761.6326\n",
            "Epoch [730/1000], Loss: 2320.8394\n",
            "Epoch [730/1000], Loss: 3032.2749\n",
            "Epoch [730/1000], Loss: 3309.792\n",
            "Epoch [730/1000], Loss: 2672.3083\n",
            "Epoch [730/1000], Loss: 2099.4954\n",
            "Epoch [730/1000], Loss: 3906.8899\n",
            "Epoch [730/1000], Loss: 3628.7046\n",
            "Epoch [730/1000], Loss: 8676.8115\n",
            "Epoch [740/1000], Loss: 2507.1294\n",
            "Epoch [740/1000], Loss: 3328.7805\n",
            "Epoch [740/1000], Loss: 2560.9131\n",
            "Epoch [740/1000], Loss: 3301.408\n",
            "Epoch [740/1000], Loss: 3368.7671\n",
            "Epoch [740/1000], Loss: 2754.354\n",
            "Epoch [740/1000], Loss: 3520.6924\n",
            "Epoch [740/1000], Loss: 2076.104\n",
            "Epoch [750/1000], Loss: 2459.7986\n",
            "Epoch [750/1000], Loss: 2985.5918\n",
            "Epoch [750/1000], Loss: 2763.2581\n",
            "Epoch [750/1000], Loss: 2378.5215\n",
            "Epoch [750/1000], Loss: 2381.2493\n",
            "Epoch [750/1000], Loss: 3121.1536\n",
            "Epoch [750/1000], Loss: 4937.5601\n",
            "Epoch [750/1000], Loss: 6599.9023\n",
            "Epoch [760/1000], Loss: 4095.8674\n",
            "Epoch [760/1000], Loss: 2804.0061\n",
            "Epoch [760/1000], Loss: 2758.2769\n",
            "Epoch [760/1000], Loss: 2731.9917\n",
            "Epoch [760/1000], Loss: 2766.7244\n",
            "Epoch [760/1000], Loss: 3059.8254\n",
            "Epoch [760/1000], Loss: 2944.7444\n",
            "Epoch [760/1000], Loss: 3563.6992\n",
            "Epoch [770/1000], Loss: 2813.3855\n",
            "Epoch [770/1000], Loss: 2706.2324\n",
            "Epoch [770/1000], Loss: 2651.7268\n",
            "Epoch [770/1000], Loss: 3806.2781\n",
            "Epoch [770/1000], Loss: 3033.073\n",
            "Epoch [770/1000], Loss: 3438.9856\n",
            "Epoch [770/1000], Loss: 2704.8511\n",
            "Epoch [770/1000], Loss: 3075.8291\n",
            "Epoch [780/1000], Loss: 3312.6658\n",
            "Epoch [780/1000], Loss: 3235.5747\n",
            "Epoch [780/1000], Loss: 2501.7175\n",
            "Epoch [780/1000], Loss: 3487.3533\n",
            "Epoch [780/1000], Loss: 3139.8462\n",
            "Epoch [780/1000], Loss: 3008.48\n",
            "Epoch [780/1000], Loss: 2475.7563\n",
            "Epoch [780/1000], Loss: 2296.373\n",
            "Epoch [790/1000], Loss: 2701.4202\n",
            "Epoch [790/1000], Loss: 2223.1885\n",
            "Epoch [790/1000], Loss: 2982.9385\n",
            "Epoch [790/1000], Loss: 3689.9812\n",
            "Epoch [790/1000], Loss: 3455.3997\n",
            "Epoch [790/1000], Loss: 3153.9668\n",
            "Epoch [790/1000], Loss: 2827.6658\n",
            "Epoch [790/1000], Loss: 4217.542\n",
            "Epoch [800/1000], Loss: 3357.6182\n",
            "Epoch [800/1000], Loss: 3004.4131\n",
            "Epoch [800/1000], Loss: 3481.3779\n",
            "Epoch [800/1000], Loss: 2538.2839\n",
            "Epoch [800/1000], Loss: 2779.4624\n",
            "Epoch [800/1000], Loss: 2769.8906\n",
            "Epoch [800/1000], Loss: 3249.3291\n",
            "Epoch [800/1000], Loss: 1196.1736\n",
            "Epoch [810/1000], Loss: 4003.6804\n",
            "Epoch [810/1000], Loss: 2307.7991\n",
            "Epoch [810/1000], Loss: 3448.6172\n",
            "Epoch [810/1000], Loss: 2570.2014\n",
            "Epoch [810/1000], Loss: 2571.1553\n",
            "Epoch [810/1000], Loss: 2771.8843\n",
            "Epoch [810/1000], Loss: 3436.4353\n",
            "Epoch [810/1000], Loss: 1562.6042\n",
            "Epoch [820/1000], Loss: 2985.3291\n",
            "Epoch [820/1000], Loss: 2874.0339\n",
            "Epoch [820/1000], Loss: 2989.25\n",
            "Epoch [820/1000], Loss: 3139.2983\n",
            "Epoch [820/1000], Loss: 3147.5334\n",
            "Epoch [820/1000], Loss: 2370.4587\n",
            "Epoch [820/1000], Loss: 3307.9658\n",
            "Epoch [820/1000], Loss: 5936.8887\n",
            "Epoch [830/1000], Loss: 2258.7871\n",
            "Epoch [830/1000], Loss: 2883.3259\n",
            "Epoch [830/1000], Loss: 3093.3567\n",
            "Epoch [830/1000], Loss: 2270.0354\n",
            "Epoch [830/1000], Loss: 4066.1121\n",
            "Epoch [830/1000], Loss: 2775.823\n",
            "Epoch [830/1000], Loss: 3661.9736\n",
            "Epoch [830/1000], Loss: 2308.3975\n",
            "Epoch [840/1000], Loss: 3617.7417\n",
            "Epoch [840/1000], Loss: 2430.4001\n",
            "Epoch [840/1000], Loss: 2788.1602\n",
            "Epoch [840/1000], Loss: 3000.3091\n",
            "Epoch [840/1000], Loss: 2358.9316\n",
            "Epoch [840/1000], Loss: 3032.4822\n",
            "Epoch [840/1000], Loss: 3774.5862\n",
            "Epoch [840/1000], Loss: 1842.6519\n",
            "Epoch [850/1000], Loss: 2814.8525\n",
            "Epoch [850/1000], Loss: 2639.7288\n",
            "Epoch [850/1000], Loss: 3607.417\n",
            "Epoch [850/1000], Loss: 2650.6318\n",
            "Epoch [850/1000], Loss: 4076.7192\n",
            "Epoch [850/1000], Loss: 2186.5991\n",
            "Epoch [850/1000], Loss: 2824.1409\n",
            "Epoch [850/1000], Loss: 4745.6465\n",
            "Epoch [860/1000], Loss: 3036.9043\n",
            "Epoch [860/1000], Loss: 3327.9968\n",
            "Epoch [860/1000], Loss: 2618.7168\n",
            "Epoch [860/1000], Loss: 2061.9524\n",
            "Epoch [860/1000], Loss: 3388.0156\n",
            "Epoch [860/1000], Loss: 3734.6748\n",
            "Epoch [860/1000], Loss: 2607.8611\n",
            "Epoch [860/1000], Loss: 4866.3755\n",
            "Epoch [870/1000], Loss: 2668.9275\n",
            "Epoch [870/1000], Loss: 2662.9167\n",
            "Epoch [870/1000], Loss: 3049.8599\n",
            "Epoch [870/1000], Loss: 2588.4241\n",
            "Epoch [870/1000], Loss: 3139.5698\n",
            "Epoch [870/1000], Loss: 2329.199\n",
            "Epoch [870/1000], Loss: 4316.356\n",
            "Epoch [870/1000], Loss: 4665.6011\n",
            "Epoch [880/1000], Loss: 2411.9897\n",
            "Epoch [880/1000], Loss: 3051.8704\n",
            "Epoch [880/1000], Loss: 2469.0881\n",
            "Epoch [880/1000], Loss: 2604.5715\n",
            "Epoch [880/1000], Loss: 3223.8162\n",
            "Epoch [880/1000], Loss: 3470.6755\n",
            "Epoch [880/1000], Loss: 3445.7161\n",
            "Epoch [880/1000], Loss: 5549.3198\n",
            "Epoch [890/1000], Loss: 2420.9543\n",
            "Epoch [890/1000], Loss: 3718.1475\n",
            "Epoch [890/1000], Loss: 2895.468\n",
            "Epoch [890/1000], Loss: 3567.0303\n",
            "Epoch [890/1000], Loss: 2467.2415\n",
            "Epoch [890/1000], Loss: 2576.1299\n",
            "Epoch [890/1000], Loss: 3040.3574\n",
            "Epoch [890/1000], Loss: 5163.1948\n",
            "Epoch [900/1000], Loss: 2635.0237\n",
            "Epoch [900/1000], Loss: 3506.7192\n",
            "Epoch [900/1000], Loss: 3291.7\n",
            "Epoch [900/1000], Loss: 3293.0486\n",
            "Epoch [900/1000], Loss: 2022.3779\n",
            "Epoch [900/1000], Loss: 2835.4084\n",
            "Epoch [900/1000], Loss: 3369.7913\n",
            "Epoch [900/1000], Loss: 260.268\n",
            "Epoch [910/1000], Loss: 3863.9424\n",
            "Epoch [910/1000], Loss: 3138.4399\n",
            "Epoch [910/1000], Loss: 3269.8025\n",
            "Epoch [910/1000], Loss: 2370.7917\n",
            "Epoch [910/1000], Loss: 2855.075\n",
            "Epoch [910/1000], Loss: 2564.3057\n",
            "Epoch [910/1000], Loss: 2713.3052\n",
            "Epoch [910/1000], Loss: 2937.7041\n",
            "Epoch [920/1000], Loss: 2741.2866\n",
            "Epoch [920/1000], Loss: 2814.7356\n",
            "Epoch [920/1000], Loss: 3303.5999\n",
            "Epoch [920/1000], Loss: 3307.1108\n",
            "Epoch [920/1000], Loss: 2563.9812\n",
            "Epoch [920/1000], Loss: 2652.4714\n",
            "Epoch [920/1000], Loss: 3466.3372\n",
            "Epoch [920/1000], Loss: 1417.2386\n",
            "Epoch [930/1000], Loss: 2832.6069\n",
            "Epoch [930/1000], Loss: 3041.1116\n",
            "Epoch [930/1000], Loss: 3493.9861\n",
            "Epoch [930/1000], Loss: 2100.231\n",
            "Epoch [930/1000], Loss: 3804.9868\n",
            "Epoch [930/1000], Loss: 3013.7925\n",
            "Epoch [930/1000], Loss: 2579.0229\n",
            "Epoch [930/1000], Loss: 683.1232\n",
            "Epoch [940/1000], Loss: 2790.325\n",
            "Epoch [940/1000], Loss: 3374.8499\n",
            "Epoch [940/1000], Loss: 2492.1064\n",
            "Epoch [940/1000], Loss: 2734.3525\n",
            "Epoch [940/1000], Loss: 2317.4822\n",
            "Epoch [940/1000], Loss: 3594.1575\n",
            "Epoch [940/1000], Loss: 3501.9324\n",
            "Epoch [940/1000], Loss: 1614.0404\n",
            "Epoch [950/1000], Loss: 2961.5505\n",
            "Epoch [950/1000], Loss: 3112.1074\n",
            "Epoch [950/1000], Loss: 2818.1851\n",
            "Epoch [950/1000], Loss: 2642.959\n",
            "Epoch [950/1000], Loss: 2574.5842\n",
            "Epoch [950/1000], Loss: 3315.4956\n",
            "Epoch [950/1000], Loss: 3388.4873\n",
            "Epoch [950/1000], Loss: 996.4585\n",
            "Epoch [960/1000], Loss: 3651.4702\n",
            "Epoch [960/1000], Loss: 2500.2847\n",
            "Epoch [960/1000], Loss: 3449.4814\n",
            "Epoch [960/1000], Loss: 3185.1409\n",
            "Epoch [960/1000], Loss: 3056.1162\n",
            "Epoch [960/1000], Loss: 2525.3022\n",
            "Epoch [960/1000], Loss: 2403.2219\n",
            "Epoch [960/1000], Loss: 1665.9691\n",
            "Epoch [970/1000], Loss: 2640.4956\n",
            "Epoch [970/1000], Loss: 2477.2361\n",
            "Epoch [970/1000], Loss: 2906.8098\n",
            "Epoch [970/1000], Loss: 2777.1399\n",
            "Epoch [970/1000], Loss: 2895.7844\n",
            "Epoch [970/1000], Loss: 3231.4705\n",
            "Epoch [970/1000], Loss: 3562.6643\n",
            "Epoch [970/1000], Loss: 6136.002\n",
            "Epoch [980/1000], Loss: 3106.7891\n",
            "Epoch [980/1000], Loss: 3319.8425\n",
            "Epoch [980/1000], Loss: 2347.0388\n",
            "Epoch [980/1000], Loss: 3161.4504\n",
            "Epoch [980/1000], Loss: 2297.9995\n",
            "Epoch [980/1000], Loss: 2784.5728\n",
            "Epoch [980/1000], Loss: 3333.0586\n",
            "Epoch [980/1000], Loss: 7968.875\n",
            "Epoch [990/1000], Loss: 2082.8906\n",
            "Epoch [990/1000], Loss: 3088.9153\n",
            "Epoch [990/1000], Loss: 2500.5977\n",
            "Epoch [990/1000], Loss: 3328.2668\n",
            "Epoch [990/1000], Loss: 2630.8354\n",
            "Epoch [990/1000], Loss: 3839.2986\n",
            "Epoch [990/1000], Loss: 3183.5798\n",
            "Epoch [990/1000], Loss: 2622.4773\n",
            "Epoch [1000/1000], Loss: 2912.3594\n",
            "Epoch [1000/1000], Loss: 1946.2334\n",
            "Epoch [1000/1000], Loss: 3580.6025\n",
            "Epoch [1000/1000], Loss: 3008.3594\n",
            "Epoch [1000/1000], Loss: 3663.1543\n",
            "Epoch [1000/1000], Loss: 2753.4424\n",
            "Epoch [1000/1000], Loss: 2865.9219\n",
            "Epoch [1000/1000], Loss: 1072.925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97fc37c5"
      },
      "source": [
        "After the model retrained with 1000 epochs, I will calculate the average Mean squared error (MSE) to numericalise the improvement.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc64824b",
        "outputId": "9bbe6157-af7e-4cd0-cac1-a15270199ff5"
      },
      "source": [
        "model.eval() # testing mode\n",
        "mse_values = [] # collect the MSE scores\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs) # predict the test data\n",
        "\n",
        "        # Calculate Mean Squared Error\n",
        "        mse = criterion(outputs, targets) # calcualte mse for the batch\n",
        "        mse_values.append(mse.item()) # add to the list of MSE values\n",
        "\n",
        "# Calculate and print the average MSE\n",
        "avg_mse = np.mean(mse_values)\n",
        "print(f\"Average MSE on test set: {avg_mse}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average MSE on test set: 2855.3446044921875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The average MSE of model with 1,000 epochs is 2855.34 whereas the average MSE from model using 100 epochs previously is 19474.77 which shows improvement in result and model perfromance."
      ],
      "metadata": {
        "id": "YkuWxbuwrNmB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b3db894"
      },
      "source": [
        "I want to visualise the predicted and actual data side by side so I will create a Dataframe.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "5c076620",
        "outputId": "54b29cf9-0c0f-4b4d-80a2-d775eee5d29e"
      },
      "source": [
        "# Evaluation\n",
        "model.eval()\n",
        "predictions = []\n",
        "actuals = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        predictions.extend(outputs.cpu().numpy())\n",
        "        actuals.extend(targets.cpu().numpy())\n",
        "\n",
        "# Create DataFrame\n",
        "results_df = pd.DataFrame({'Predicted': np.array(predictions).flatten(), 'Actual': np.array(actuals).flatten()})\n",
        "results_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Predicted  Actual\n",
              "0   146.186325   219.0\n",
              "1   175.400375    70.0\n",
              "2   140.885605   202.0\n",
              "3   295.655121   230.0\n",
              "4   127.976288   111.0\n",
              "..         ...     ...\n",
              "84  116.307327   153.0\n",
              "85   88.178833    98.0\n",
              "86   76.428345    37.0\n",
              "87   66.233948    63.0\n",
              "88  153.988846   184.0\n",
              "\n",
              "[89 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e72886e1-198d-4bfc-baf2-7d92646453fe\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted</th>\n",
              "      <th>Actual</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>146.186325</td>\n",
              "      <td>219.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>175.400375</td>\n",
              "      <td>70.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>140.885605</td>\n",
              "      <td>202.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>295.655121</td>\n",
              "      <td>230.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>127.976288</td>\n",
              "      <td>111.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>116.307327</td>\n",
              "      <td>153.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>88.178833</td>\n",
              "      <td>98.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>76.428345</td>\n",
              "      <td>37.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>66.233948</td>\n",
              "      <td>63.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>153.988846</td>\n",
              "      <td>184.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>89 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e72886e1-198d-4bfc-baf2-7d92646453fe')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e72886e1-198d-4bfc-baf2-7d92646453fe button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e72886e1-198d-4bfc-baf2-7d92646453fe');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_5ca2eca4-a73e-4162-a6e3-b47a8d307d21\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_5ca2eca4-a73e-4162-a6e3-b47a8d307d21 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('results_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "results_df",
              "summary": "{\n  \"name\": \"results_df\",\n  \"rows\": 89,\n  \"fields\": [\n    {\n      \"column\": \"Predicted\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 89,\n        \"samples\": [\n          176.2816619873047,\n          111.51869201660156,\n          177.9231414794922\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Actual\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          111.0,\n          61.0,\n          252.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ]
}